{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a222dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What we will cover today?\n",
    "## Numerical Instability in Programs (why it's important to know)?\n",
    "## Two examples of numerical instability: Softmax, F1Score (Dice Score)\n",
    "\n",
    "## Exploring the nn Module\n",
    "## torch.eye, torch.ones, torch.zeros etc\n",
    "## Dropout and 1/(1-p)\n",
    "## MaxPool\n",
    "## conv1D and np.convolve ? (which one is correct)\n",
    "## BatchNorm (only the very basics, no talk on running mean , shift and scale for now)\n",
    "## Class way of doing dropout , maxpool and BatchNorm\n",
    "## nn.Linear is just dot product\n",
    "\n",
    "## We will discuss in detail but please do a small research on these 4\n",
    "## Many losses: which one to pick (in classification, BCEwithLogitsLoss, CrossEntropyLoss)\n",
    "## NLLLoss and BCELoss (don't use them)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef9a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c38735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sigmoid and softmax\n",
    "## Hardmax????\n",
    "\n",
    "t1 = torch.tensor([[1.0,2.1,3.2], \n",
    "                   [3.2, 4.2, 5.65],\n",
    "                   [3.6, 7.3, 1.2],\n",
    "                   [1.2, 1.65, 4.2]])\n",
    "\n",
    "t2 = torch.tensor([[100.0,200.1,30.2], \n",
    "                   [3.2, 4.2, 5.65],\n",
    "                   [3.6, 7.3, 1.2],\n",
    "                   [1.2, 1.65, 4.2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5a42651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  24.5325,  284.2915, 1480.3002,   66.6863])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(t1.max(dim=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f5771c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## row\n",
    "## sample\n",
    "## observations\n",
    "## Feature Vector\n",
    "## Feature Vector != Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7109b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.1000, 3.2000],\n",
       "        [3.2000, 4.2000, 5.6500],\n",
       "        [3.6000, 7.3000, 1.2000],\n",
       "        [1.2000, 1.6500, 4.2000]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb2d0d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28fdd922",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119d1970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.activation.Softmax"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c880b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0768, 0.2306, 0.6927],\n",
       "        [0.0653, 0.1776, 0.7571],\n",
       "        [0.0241, 0.9737, 0.0022],\n",
       "        [0.0441, 0.0692, 0.8866]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c6bce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 2.1000, 3.2000])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81ec2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iexp = torch.exp(t1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab7e0929",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_exp = iexp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4889d3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0768, 0.2306, 0.6927])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iexp/tot_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "179616fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(arr):\n",
    "    iexp = torch.exp(arr - arr.max())\n",
    "    tot_exp = iexp.sum()\n",
    "    return iexp/tot_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41c35e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0768, 0.2306, 0.6927])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax(t1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10c1898e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.0768, 0.2306, 0.6927]),\n",
       " tensor([0.0653, 0.1776, 0.7571]),\n",
       " tensor([0.0241, 0.9737, 0.0022]),\n",
       " tensor([0.0441, 0.0692, 0.8866])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[my_softmax(i) for i in t1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9df0e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0768, 0.2306, 0.6927],\n",
       "        [0.0653, 0.1776, 0.7571],\n",
       "        [0.0241, 0.9737, 0.0022],\n",
       "        [0.0441, 0.0692, 0.8866]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "18356500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3631e-44, 1.0000e+00, 0.0000e+00],\n",
       "        [6.5331e-02, 1.7759e-01, 7.5708e-01],\n",
       "        [2.4074e-02, 9.7374e-01, 2.1840e-03],\n",
       "        [4.4143e-02, 6.9229e-02, 8.8663e-01]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9932fe23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.3631e-44, 1.0000e+00, 0.0000e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_softmax(t2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e8ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2*recall*precision/ (precision  + recall) => numerically unstable\n",
    "## precision , recall => ratio (unstable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1147b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2*TP/ (2TP + FP + FN) => numericall stable\n",
    "## a/(b + absalen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exploring the nn Module\n",
    "## torch.eye, torch.ones, torch.zeros etc\n",
    "## Dropout and 1/(1-p)\n",
    "## MaxPool\n",
    "## conv1D and np.convolve ? (which one is correct)\n",
    "## BatchNorm (only the very basics, no talk on running mean , shift and scale for now)\n",
    "## Class way of doing dropout , maxpool and BatchNorm\n",
    "## nn.Linear is just dot product\n",
    "\n",
    "## We will discuss in detail but please do a small research on these 4\n",
    "## Many losses: which one to pick (in classification, BCEwithLogitsLoss, CrossEntropyLoss)\n",
    "## NLLLoss and BCELoss (don't use them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "806ffdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrix \n",
    "n = (torch.eye(10)*100).clone().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a143cb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e4d6c84080>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(n, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c58e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.ones((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44a272c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.zeros((10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f2902bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24818317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "cca02525",
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn\n",
    "t1 = torch.rand(10, 10)\n",
    "d = nn.Dropout(p=.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e4d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6d101d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5130, 0.7152, 0.8709, 0.3323, 0.2042, 0.1139, 0.3172, 0.7565, 0.3315,\n",
       "         0.8292],\n",
       "        [0.1614, 0.6397, 0.8050, 0.2245, 0.3374, 0.2272, 0.9115, 0.5536, 0.7454,\n",
       "         0.2701],\n",
       "        [0.0359, 0.4352, 0.4700, 0.4978, 0.5348, 0.1008, 0.1694, 0.2979, 0.8720,\n",
       "         0.5741],\n",
       "        [0.7854, 0.7565, 0.8606, 0.2602, 0.4868, 0.6396, 0.1681, 0.5728, 0.3247,\n",
       "         0.8846],\n",
       "        [0.8365, 0.0269, 0.2599, 0.4153, 0.8825, 0.8099, 0.1458, 0.3494, 0.8757,\n",
       "         0.7711],\n",
       "        [0.3628, 0.4100, 0.4408, 0.5117, 0.2880, 0.0853, 0.6623, 0.2780, 0.1513,\n",
       "         0.9855],\n",
       "        [0.2048, 0.8826, 0.7474, 0.1248, 0.3582, 0.3488, 0.7757, 0.2839, 0.6572,\n",
       "         0.7310],\n",
       "        [0.1648, 0.9323, 0.0813, 0.6364, 0.1457, 0.0414, 0.9672, 0.2611, 0.0758,\n",
       "         0.6401],\n",
       "        [0.6813, 0.9976, 0.4676, 0.2125, 0.7711, 0.9144, 0.2267, 0.4684, 0.4221,\n",
       "         0.1058],\n",
       "        [0.9734, 0.9499, 0.4303, 0.2087, 0.9945, 0.8116, 0.5876, 0.1681, 0.2254,\n",
       "         0.7578]])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0efc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "6529a3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 1.4304, 1.7419, 0.6645, 0.0000, 0.0000, 0.0000, 0.0000, 0.6629,\n",
       "         0.0000],\n",
       "        [0.3229, 0.0000, 0.0000, 0.0000, 0.0000, 0.4545, 0.0000, 0.0000, 1.4909,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9956, 1.0696, 0.2016, 0.3389, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 1.5131, 0.0000, 0.5204, 0.9736, 1.2792, 0.0000, 1.1456, 0.0000,\n",
       "         0.0000],\n",
       "        [1.6730, 0.0538, 0.0000, 0.0000, 1.7650, 0.0000, 0.2916, 0.6988, 1.7515,\n",
       "         1.5421],\n",
       "        [0.0000, 0.8201, 0.0000, 0.0000, 0.5760, 0.1706, 1.3245, 0.5560, 0.3025,\n",
       "         0.0000],\n",
       "        [0.4096, 0.0000, 1.4949, 0.2497, 0.0000, 0.6977, 0.0000, 0.5679, 1.3145,\n",
       "         1.4621],\n",
       "        [0.3296, 1.8645, 0.0000, 0.0000, 0.0000, 0.0828, 1.9345, 0.5221, 0.0000,\n",
       "         1.2802],\n",
       "        [0.0000, 1.9951, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2116],\n",
       "        [0.0000, 0.0000, 0.0000, 0.4175, 1.9891, 0.0000, 0.0000, 0.0000, 0.4509,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "51a34a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1 / (1 - .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c3c15795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "efa2a288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0946, 1.6576, 1.923)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5473*2, 0.8288*2, 0.9615*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2f49b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1d = t1.unsqueeze(0).unsqueeze(0).clone().detach()\n",
    "t2d = t1.unsqueeze(0).unsqueeze(0).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38ac9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "7f16ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t12 = torch.column_stack((t1d, t2d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "63098f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t12 = t12.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4d7ec132",
   "metadata": {},
   "outputs": [],
   "source": [
    "t13 = t12.clone().detach().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e0b3dec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t13.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "12516052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 8, 8])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t12.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "a29a6c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d56c49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Dropout1d(p=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "33aaf780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout1d(p=0.5, inplace=False)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f72a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "## forward =>  __call__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ce7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1e042cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = m(t13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "47103c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3324, 1.5402, 0.1077, 0.6394, 0.8962, 0.6196, 1.0837, 1.7591],\n",
       "         [1.8537, 0.0751, 1.0980, 0.3911, 0.7586, 0.4804, 0.6934, 0.4517],\n",
       "         [1.5290, 1.1432, 1.7224, 1.2094, 0.1907, 1.9332, 0.2783, 0.2709]],\n",
       "\n",
       "        [[0.6463, 0.3125, 0.9787, 1.0725, 1.0707, 0.7716, 1.1336, 0.5600],\n",
       "         [0.3989, 1.9588, 0.4075, 0.8432, 0.4353, 1.9093, 0.4914, 1.8998],\n",
       "         [1.3730, 0.9991, 1.8692, 1.5935, 0.7600, 1.0777, 0.9981, 1.1337],\n",
       "         [0.5194, 0.7436, 1.6062, 0.7516, 0.7900, 1.7678, 0.3413, 0.5625],\n",
       "         [0.2120, 1.4464, 1.7058, 0.5403, 1.1585, 1.1275, 0.8800, 0.9706],\n",
       "         [0.3324, 1.5402, 0.1077, 0.6394, 0.8962, 0.6196, 1.0837, 1.7591],\n",
       "         [1.8537, 0.0751, 1.0980, 0.3911, 0.7586, 0.4804, 0.6934, 0.4517],\n",
       "         [1.5290, 1.1432, 1.7224, 1.2094, 0.1907, 1.9332, 0.2783, 0.2709]]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d8a9481c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False]]],\n",
       "\n",
       "\n",
       "        [[[False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False],\n",
       "          [False, False, False, False, False, False, False, False]]]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.isclose(m2, torch.tensor(0,dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drop(nn.Module):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super().__init__()\n",
    "        self.x = x\n",
    "\n",
    "    def forward(self, dropout=.25):\n",
    "        return nn.Dropout(self.x, dropout)\n",
    "    ## this is wrong and will never work\n",
    "\n",
    "## Rules:\n",
    "# Inputs must go to forward()\n",
    "# Layers / hyperparameters go in __init__()\n",
    "# example fails because it violates both rules and misuses nn.Dropout.\n",
    "\n",
    "## So we will fix this by doing this\n",
    "\n",
    "class Drop(nn.Module):\n",
    "    def __init__(self, p=0.25):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d4e313",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "fbdfdd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e114d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = torch.tensor(x).clone().detach()\n",
    "kt = torch.tensor(k).clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9a398216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m       np.convolve(a, v, mode=\u001b[33m'full'\u001b[39m)\n",
      "\u001b[31mCall signature:\u001b[39m  np.convolve(*args, **kwargs)\n",
      "\u001b[31mType:\u001b[39m            _ArrayFunctionDispatcher\n",
      "\u001b[31mString form:\u001b[39m     <function convolve at 0x000001E4ACD622A0>\n",
      "\u001b[31mFile:\u001b[39m            d:\\learning_pytorch\\pytorch_session\\pytorch_sessions\\.venv\\lib\\site-packages\\numpy\\_core\\numeric.py\n",
      "\u001b[31mDocstring:\u001b[39m      \n",
      "Returns the discrete, linear convolution of two one-dimensional sequences.\n",
      "\n",
      "The convolution operator is often seen in signal processing, where it\n",
      "models the effect of a linear time-invariant system on a signal [1]_.  In\n",
      "probability theory, the sum of two independent random variables is\n",
      "distributed according to the convolution of their individual\n",
      "distributions.\n",
      "\n",
      "If `v` is longer than `a`, the arrays are swapped before computation.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "a : (N,) array_like\n",
      "    First one-dimensional input array.\n",
      "v : (M,) array_like\n",
      "    Second one-dimensional input array.\n",
      "mode : {'full', 'valid', 'same'}, optional\n",
      "    'full':\n",
      "      By default, mode is 'full'.  This returns the convolution\n",
      "      at each point of overlap, with an output shape of (N+M-1,). At\n",
      "      the end-points of the convolution, the signals do not overlap\n",
      "      completely, and boundary effects may be seen.\n",
      "\n",
      "    'same':\n",
      "      Mode 'same' returns output of length ``max(M, N)``.  Boundary\n",
      "      effects are still visible.\n",
      "\n",
      "    'valid':\n",
      "      Mode 'valid' returns output of length\n",
      "      ``max(M, N) - min(M, N) + 1``.  The convolution product is only given\n",
      "      for points where the signals overlap completely.  Values outside\n",
      "      the signal boundary have no effect.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "out : ndarray\n",
      "    Discrete, linear convolution of `a` and `v`.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "scipy.signal.fftconvolve : Convolve two arrays using the Fast Fourier\n",
      "                           Transform.\n",
      "scipy.linalg.toeplitz : Used to construct the convolution operator.\n",
      "polymul : Polynomial multiplication. Same output as convolve, but also\n",
      "          accepts poly1d objects as input.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The discrete convolution operation is defined as\n",
      "\n",
      ".. math:: (a * v)_n = \\sum_{m = -\\infty}^{\\infty} a_m v_{n - m}\n",
      "\n",
      "It can be shown that a convolution :math:`x(t) * y(t)` in time/space\n",
      "is equivalent to the multiplication :math:`X(f) Y(f)` in the Fourier\n",
      "domain, after appropriate padding (padding is necessary to prevent\n",
      "circular convolution).  Since multiplication is more efficient (faster)\n",
      "than convolution, the function `scipy.signal.fftconvolve` exploits the\n",
      "FFT to calculate the convolution of large data-sets.\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] Wikipedia, \"Convolution\",\n",
      "    https://en.wikipedia.org/wiki/Convolution\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Note how the convolution operator flips the second array\n",
      "before \"sliding\" the two across one another:\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> np.convolve([1, 2, 3], [0, 1, 0.5])\n",
      "array([0. , 1. , 2.5, 4. , 1.5])\n",
      "\n",
      "Only return the middle values of the convolution.\n",
      "Contains boundary effects, where zeros are taken\n",
      "into account:\n",
      "\n",
      ">>> np.convolve([1,2,3],[0,1,0.5], 'same')\n",
      "array([1. ,  2.5,  4. ])\n",
      "\n",
      "The two arrays are of the same length, so there\n",
      "is only one position where they completely overlap:\n",
      "\n",
      ">>> np.convolve([1,2,3],[0,1,0.5], 'valid')\n",
      "array([2.5])\n",
      "\u001b[31mClass docstring:\u001b[39m\n",
      "Class to wrap functions with checks for __array_function__ overrides.\n",
      "\n",
      "All arguments are required, and can only be passed by position.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "dispatcher : function or None\n",
      "    The dispatcher function that returns a single sequence-like object\n",
      "    of all arguments relevant.  It must have the same signature (except\n",
      "    the default values) as the actual implementation.\n",
      "    If ``None``, this is a ``like=`` dispatcher and the\n",
      "    ``_ArrayFunctionDispatcher`` must be called with ``like`` as the\n",
      "    first (additional and positional) argument.\n",
      "implementation : function\n",
      "    Function that implements the operation on NumPy arrays without\n",
      "    overrides.  Arguments passed calling the ``_ArrayFunctionDispatcher``\n",
      "    will be forwarded to this (and the ``dispatcher``) as if using\n",
      "    ``*args, **kwargs``.\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "_implementation : function\n",
      "    The original implementation passed in."
     ]
    }
   ],
   "source": [
    "np.convolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ac30a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 16, 22, 28])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.convolve(x, k, 'valid') ## correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "13fefc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 20, 26, 32])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.convolve(x, k[::-1], 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d513833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[14, 20, 26, 32]]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.conv1d(xt.reshape(1, 1, -1), kt.reshape(1,1,-1)) ## cross correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80ddf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## x*k[n-k] ## convolution\n",
    "## x*k[n+k] ## cross correlation\n",
    "## kernel trick :=> svm\n",
    "torch.tensor([1,2,3]) @ torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c8d9a30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([2,3,4]) @ torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "879b9682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(26)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3,4,5]) @ torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e99bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FFT => Fast Fourier Transform\n",
    "## Frequency Space\n",
    "## convolution / element wise multiplication => beyond my brain ()\n",
    "## How convolution works in frequency space (with pure maths)\n",
    "\n",
    "## MaxPool\n",
    "## BatchNorm ( scale, shift, rolling mean, rolling std)\n",
    "\n",
    "## \n",
    "\n",
    "## softmax , f1 score => numerically unstable, normalise\n",
    "## dropuut => scale => inference => weights same scale\n",
    "## confusion => droput => element wise => 1 whole tensor , 1 row per matrix\n",
    "## convolution => x*h[n-k](convolution/numpy) vs x*h[n+k] (cross correlation/pytorch)\n",
    "## convolution => implement => FFT (theory) => math (way too difficult)\n",
    "## convolution => kernel (learnable: trainable)\n",
    "## class pytorch => init (hyperparameter mention, trainable / untrainable , never mention data)\n",
    "## forward() only keeps data \n",
    "\n",
    "## In future we will implement the following\n",
    "## iris  (stdardize the data using batchnorm etc, then followed by some augmentation - details later)\n",
    "## mnist (deep learning)\n",
    "## \n",
    "## Try implementing the convolution at your end\n",
    "## convolution (pure python / fold unfold in pytorch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
